name: airflow-integration
on: [push]
jobs:
  integrate-aiflow:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v2

      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.9

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip wheel
          pip install flake8 pytest black boto3
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Run Flake8 Lint Check
        run: |
          # Stop build if any of the follow error/warning groups trigger
          flake8 . \
            --count \
            --show-source \
            --statistics \
            --max-line-length=127 \
            --exclude=__init__.py

      #- name: Run Test-Suite
        #run: |
          #if [ -f .env.sample ]; then cp .env.sample .env; fi
          #pytest

      - name: Run Black Reformatter
        run |
          # Stop build if black would need to reformat a file
          black . \
            --line-length 120 \
            --target-version py39

      - name: Zip Plugins
        run: |
          cd plugins
          chmod -R 755 .
          zip -r plugins.zip .
          mv plugins.zip ../.
          cd ..

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_AIRFLOW_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_AIRFLOW_ACCESS_SECRET_KEY }}
          aws-region: ${{ secrets.AWS_AIRFLOW_REGION }}

      - name: Upload to S3
        run: |
          aws s3 cp plugins.zip s3://airflow-gar-testing/plugins.zip
          aws s3 cp requirements.txt s3://airflow-gar-testing/requirements.txt
          aws s3 sync ./dags/ s3://airflow-gar-testing/dags/ --exclude="*" --include="*.py"

      - name: Update AWS Airflow S3 Object Versions
        shell: python
        run: |
          import os
          from time import sleep

          import boto3

          access_key_id = os.environ["AWS_AIRFLOW_ACCESS_KEY_ID"]
          access_secret_key = os.environ["AWS_AIRFLOW_ACCESS_SECRET_KEY"]
          bucket_name = os.environ["AWS_AIRFLOW_S3_BUCKET"]
          mwaa_env_name = os.environ["AWS_AIRFLOW_MWAA_ENV_NAME"]
          region = os.environ["AWS_AIRFLOW_REGION"]

          session = boto3.Session(
            aws_access_key_id=access_key_id,
            aws_secret_access_key=access_secret_key,
            region_name=region,
          )

          mwaa = session.client("mwaa")
          s3 = session.resource("s3")
          bucket = s3.Bucket(bucket_name)

          def get_latest_version(prefix):
            versions = bucket.object_versions.filter(Prefix=prefix)
            for index, item in enumerate(versions):
              if not index:
                obj = item.get()
                return obj.get("VersionId")

          def get_environment_status():
            resp = mwaa.get_environment(Name=mwaa_env_name)
            data = resp.get("Environment", {})
            return data.get("LastUpdate", {}).get("Status", "PENDING")

          while get_environment_status() == "PENDING":
            sleep(60)

          mwaa.update_environment(
            Name=mwaa_env_name,
            PluginsS3ObjectVersion=get_latest_version(prefix="plugins.zip"),
            RequirementsS3ObjectVersion=get_latest_version(prefix="requirements.txt"),
          )
